# 4.10 RAG 系统搭建实战

RAG（Retrieval-Augmented Generation，检索增强生成）解决的是一个核心问题：让大模型基于你自己的数据来回答问题，而不是凭空瞎编。企业内部文档问答、知识库搜索、客服系统——底层都是RAG。

这一章我们从零搭建一个完整的RAG系统。代码基于Python，用的都是生产级组件，不是demo级别的玩具。

## 技术选型

直接给我的推荐，不绕弯子：

| 组件 | 推荐方案 | 理由 |
|------|---------|------|
| 嵌入模型 | OpenAI `text-embedding-3-small` | 性价比高，维度1536，效果够用 |
| 向量数据库 | ChromaDB | 开源、轻量级、嵌入式友好、Python SDK简单 |
| 文档处理 | LangChain 的 Document Loaders + Text Splitters | 生态最全，支持几十种文档格式 |
| LLM | GPT-4o 或 Claude Sonnet | 根据你的API选 |
| 框架 | LangChain（做原型），生产环境建议自己封装核心逻辑 | LangChain的抽象层太厚，出问题不好调试 |

如果你的数据全是中文，嵌入模型也可以考虑用 `BAAI/bge-large-zh-v1.5`（开源，可本地部署），中文效果会更好一些。

先把依赖装上：

```bash
pip install langchain langchain-openai langchain-community chromadb openai
```

ChromaDB 可以直接作为 Python 库运行，默认使用本地持久化模式（数据保存在本地目录）。


## Step 1: 知识文档处理与分块

RAG的质量80%取决于分块（chunking）质量。分块太大，检索精度低；分块太小，上下文不完整。

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path


def load_documents(docs_dir: str) -> list:
    """加载目录下的所有文档"""
    loaders_map = {
        ".pdf": PyPDFLoader,
        ".txt": TextLoader,
        ".md": UnstructuredMarkdownLoader,
    }

    documents = []
    for file_path in Path(docs_dir).rglob("*"):
        suffix = file_path.suffix.lower()
        loader_class = loaders_map.get(suffix)
        if loader_class:
            try:
                loader = loader_class(str(file_path))
                docs = loader.load()
                # 给每个文档加上来源信息，后面做引用溯源用
                for doc in docs:
                    doc.metadata["source"] = str(file_path)
                documents.extend(docs)
                print(f"已加载: {file_path} ({len(docs)} 页)")
            except Exception as e:
                print(f"加载失败: {file_path}, 错误: {e}")

    return documents


def split_documents(documents: list) -> list:
    """将文档切分为合适大小的块"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,         # 每块约500字符
        chunk_overlap=50,       # 相邻块重叠50字符，保持上下文连贯
        separators=["\n\n", "\n", "。", "；", " ", ""],  # 中文友好的分隔符
        length_function=len,
    )

    chunks = splitter.split_documents(documents)
    print(f"文档总数: {len(documents)}, 分块后: {len(chunks)} 个块")
    return chunks


# 使用
documents = load_documents("./knowledge_base")
chunks = split_documents(documents)

# 看看分块结果
for i, chunk in enumerate(chunks[:3]):
    print(f"\n--- 块 {i} ---")
    print(f"来源: {chunk.metadata.get('source', 'unknown')}")
    print(f"内容: {chunk.page_content[:200]}...")
```

几个分块的经验：

- `chunk_size=500` 对中文文档是个不错的起点，大约250个汉字。如果你的文档段落结构清晰，可以适当调大到800-1000。
- `chunk_overlap` 设为chunk_size的10%左右，防止关键信息被切断。
- 分隔符顺序很重要，优先按段落切，实在不行才按句子切。

## Step 2: 向量化与入库

把文本块转成向量，存进ChromaDB。

```python
from openai import OpenAI
import chromadb
from chromadb.config import Settings
import uuid


class VectorStore:
    def __init__(
        self,
        collection_name: str = "knowledge_base",
        chroma_path: str = "./chroma_db",
    ):
        self.openai = OpenAI(api_key="sk-your-key-here")
        # 使用本地持久化模式，数据存在指定目录下
        self.client = chromadb.PersistentClient(path=chroma_path)
        self.collection_name = collection_name
        self.embedding_model = "text-embedding-3-small"
        
        # 获取或创建 collection
        # metadata={"hnsw:space": "cosine"} 指定使用余弦相似度
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name, 
            metadata={"hnsw:space": "cosine"}
        )

    def _get_embeddings(self, texts: list[str]) -> list[list[float]]:
        """批量获取文本的嵌入向量"""
        response = self.openai.embeddings.create(
            model=self.embedding_model,
            input=texts,
        )
        return [item.embedding for item in response.data]

    def index_documents(self, chunks: list, batch_size: int = 100):
        """将文档块向量化并存入ChromaDB"""
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            texts = [chunk.page_content for chunk in batch]
            embeddings = self._get_embeddings(texts)
            
            ids = [str(uuid.uuid4()) for _ in batch]
            metadatas = [
                {
                    "source": chunk.metadata.get("source", ""),
                    "page": chunk.metadata.get("page", 0),
                }
                for chunk in batch
            ]

            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
            )
            print(f"已入库: {i + len(batch)}/{len(chunks)}")

    def search(self, query: str, top_k: int = 5) -> list[dict]:
        """根据query检索最相关的文档块"""
        query_embedding = self._get_embeddings([query])[0]
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
        )
        
        # Chroma返回的是 List[List]，我们需要解包
        if not results["ids"] or len(results["ids"][0]) == 0:
            return []
            
        # 结果整理
        parsed_results = []
        for text, metadata, distance in zip(
            results["documents"][0], 
            results["metadatas"][0], 
            results["distances"][0]
        ):
            parsed_results.append({
                "text": text,
                "source": metadata["source"],
                # Chroma cosine distance (0~2), 转换成相似度分数 (1-distance)
                "score": 1 - distance,
            })
            
        return parsed_results


# 入库
store = VectorStore()
store.index_documents(chunks)
```

## Step 3: 检索与生成

把检索结果拼进Prompt，让LLM基于这些内容来回答。

```python
from openai import OpenAI


class RAGEngine:
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.llm = OpenAI(api_key="sk-your-key-here")

    def answer(self, question: str, top_k: int = 5) -> dict:
        """RAG问答：检索 -> 拼接上下文 -> 生成回答"""

        # 1. 检索相关文档
        results = self.vector_store.search(question, top_k=top_k)

        if not results:
            return {
                "answer": "没有找到相关信息，无法回答这个问题。",
                "sources": [],
            }

        # 2. 拼接检索到的内容作为上下文
        context_parts = []
        for i, r in enumerate(results, 1):
            context_parts.append(
                f"[文档{i}] (来源: {r['source']}, 相关度: {r['score']:.2f})\n{r['text']}"
            )
        context = "\n\n".join(context_parts)

        # 3. 构建Prompt
        system_prompt = """你是一个企业知识库问答助手。根据提供的参考文档回答用户的问题。

要求：
- 只基于提供的参考文档来回答，不要编造信息
- 如果参考文档中没有相关信息，明确告诉用户"根据现有资料无法回答"
- 在回答末尾标注信息来源（引用的文档编号）
- 回答要简洁、准确、有条理"""

        user_prompt = f"""参考文档：
{context}

用户问题：{question}"""

        # 4. 调用LLM生成回答
        response = self.llm.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,  # 知识问答场景用低temperature，减少幻觉
        )

        return {
            "answer": response.choices[0].message.content,
            "sources": [r["source"] for r in results],
            "relevance_scores": [r["score"] for r in results],
        }


# 使用
rag = RAGEngine(store)
result = rag.answer("公司的年假政策是怎样的？")
print(result["answer"])
print(f"\n参考来源: {result['sources']}")
```

把这些组合起来，包一个FastAPI接口就能对外提供服务了：

```python
from fastapi import FastAPI

app = FastAPI()
store = VectorStore()
rag = RAGEngine(store)


@app.post("/api/ask")
async def ask(request: dict):
    question = request.get("question", "")
    result = rag.answer(question)
    return {
        "code": 0,
        "data": {
            "answer": result["answer"],
            "sources": result["sources"],
        },
    }
```

## Step 4: 评估与优化

系统能跑起来只是第一步。要做到好用，需要持续评估和优化。

### 评估维度

准备一批测试问题和标准答案，从三个维度评估：

1. **检索准确率** — 检索出的前5条结果中，包含正确答案的比例。如果这个值低，说明分块或嵌入有问题。
2. **回答忠实度** — 生成的回答是否忠于检索到的文档，有没有编造信息。
3. **回答完整性** — 回答是否覆盖了问题的所有方面。

### 常见优化方向

**检索质量差 -> 优化分块策略**
- 试试按语义分块而不是固定大小分块
- 对标题和正文做不同处理，标题也嵌入进元数据

**检索到了但回答不对 -> 优化Prompt**
- 在system prompt里强调"只基于文档回答"
- 把temperature调到0.1甚至0

**长文档表现差 -> 加入重排序**
- 先检索top 20，再用Cross-Encoder重排序取top 5
- 能显著提升长尾query的效果

**用户问的方式多样 -> 查询改写**
- 在检索前先用LLM把用户的口语化问题改写为更适合检索的形式
- 比如"年假怎么请"改写为"年假申请流程 审批规则"

## 企业内部文档问答系统架构建议

真正落地到企业环境，光有核心RAG逻辑不够。这是一个经过实践验证的架构：

```
                    ┌─────────────┐
                    │   Web前端    │
                    └──────┬──────┘
                           │
                    ┌──────▼──────┐
                    │  API Gateway │  认证、限流、日志
                    └──────┬──────┘
                           │
                    ┌──────▼──────┐
                    │  RAG Service │  核心问答服务
                    └──┬───┬───┬──┘
                       │   │   │
            ┌──────────┘   │   └──────────┐
            │              │              │
     ┌──────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
     │  向量数据库   │ │  LLM API │ │  文档管理服务 │
     │  (ChromaDB) │ │          │ │  上传/更新/删除│
     └─────────────┘ └──────────┘ └──────┬──────┘
                                         │
                                  ┌──────▼──────┐
                                  │  文档处理管道 │
                                  │  解析→分块→入库│
                                  └─────────────┘
```

几个架构层面的关键决策：

1. **文档更新机制** — 文档不是入库一次就完事了。要有增量更新能力：文档变更时删掉旧的chunks，重新分块入库。用文档的hash值做变更检测。

2. **权限控制** — 企业文档有密级的。在chunk的metadata里记录文档的权限标签，检索时根据用户角色过滤。不要让实习生能搜到董事会纪要。

3. **对话记忆** — 用户会追问。把对话历史也带进Prompt，但要控制长度，最近5轮对话就够了。

4. **反馈闭环** — 加一个"这个回答有用吗"的反馈按钮。收集到的数据是优化系统最宝贵的输入。

5. **监控告警** — 监控检索命中率、LLM响应时间、token消耗。设置异常告警，比如某类问题的检索得分一直很低，说明知识库需要补充。
