# 4.12 AI DevOps 实战

AI在DevOps领域的应用可能是被低估最多的方向。不是让AI帮你搭CI/CD——那些流程已经很成熟了。而是让AI帮你干那些最耗时的脏活：读日志、分析报错、排查故障、优化SQL。

这一章全是实战模板，你可以直接拿去用。

## AI 辅助 CI/CD：构建失败分析

CI构建挂了，一坨红色日志丢过来，人肉看日志是最浪费工程师时间的事情之一。让AI来干。

### Prompt 模板：构建失败诊断

```
你是一个CI/CD构建问题诊断专家。分析以下构建失败日志，给出诊断结果。

构建信息：
- 项目：{{project_name}}
- 分支：{{branch}}
- 构建工具：{{build_tool}} (如 GitHub Actions / Jenkins / GitLab CI)
- 触发方式：{{trigger}} (如 push / PR / 定时)

失败日志（已截取关键部分）：
```
{{build_log}}
```

请按以下格式回答：

1. 根因分析：一句话说明构建失败的直接原因
2. 错误分类：(编译错误 / 依赖问题 / 测试失败 / 环境问题 / 配置错误 / 资源不足)
3. 修复方案：给出具体的修复步骤，包括需要修改的文件和命令
4. 预防建议：怎么避免下次再出现类似问题
```

### 自动化集成脚本

在CI pipeline里加一个步骤，构建失败时自动调用AI分析，把结果发到群里：

```python
#!/usr/bin/env python3
"""
ci_failure_analyzer.py
用法：在CI的 on_failure 步骤中调用
  python ci_failure_analyzer.py --log-file build.log --project my-app --branch main
"""
import argparse
import json
import os

import httpx
from openai import OpenAI


def analyze_build_failure(log_content: str, project: str, branch: str) -> str:
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    # 日志太长时截取最后500行（通常错误在末尾）
    lines = log_content.strip().split("\n")
    if len(lines) > 500:
        log_content = "\n".join(lines[-500:])

    prompt = f"""你是一个CI/CD构建问题诊断专家。分析以下构建失败日志，给出诊断结果。

构建信息：
- 项目：{project}
- 分支：{branch}

失败日志：
```
{log_content}
```

请按以下格式回答：
1. 根因分析：一句话说明失败原因
2. 错误分类：(编译错误/依赖问题/测试失败/环境问题/配置错误/资源不足)
3. 修复方案：具体步骤
4. 预防建议"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=1500,
    )

    return response.choices[0].message.content


def send_to_webhook(message: str, webhook_url: str):
    """发送到企业微信/飞书/Slack webhook"""
    # 以飞书为例
    payload = {
        "msg_type": "text",
        "content": {"text": f"[CI构建失败分析]\n\n{message}"},
    }
    httpx.post(webhook_url, json=payload, timeout=10)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--log-file", required=True)
    parser.add_argument("--project", required=True)
    parser.add_argument("--branch", default="main")
    args = parser.parse_args()

    with open(args.log_file, "r") as f:
        log_content = f.read()

    analysis = analyze_build_failure(log_content, args.project, args.branch)
    print(analysis)

    # 如果配置了webhook，自动发送
    webhook_url = os.environ.get("NOTIFY_WEBHOOK_URL")
    if webhook_url:
        send_to_webhook(analysis, webhook_url)


if __name__ == "__main__":
    main()
```

在 GitHub Actions 中集成：

```yaml
# .github/workflows/build.yml
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build
        run: npm run build 2>&1 | tee build.log

      - name: AI分析构建失败
        if: failure()
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          NOTIFY_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK }}
        run: |
          python ci_failure_analyzer.py \
            --log-file build.log \
            --project ${{ github.repository }} \
            --branch ${{ github.ref_name }}
```

## AI 辅助运维排障

### Prompt 模板：日志异常分析

```
你是一个资深SRE工程师。分析以下应用日志，识别异常模式并给出排障方向。

应用信息：
- 服务名：{{service_name}}
- 运行环境：{{environment}} (如 production / staging)
- 技术栈：{{tech_stack}} (如 Java Spring Boot / Node.js Express)
- 当前症状：{{symptom}} (如 接口超时增多 / 5xx激增 / OOM重启)

日志片段（最近30分钟）：
```
{{log_content}}
```

请按以下维度分析：

1. 异常模式识别
   - 出现了哪些异常类型
   - 异常的频率和时间分布
   - 是否有关联性（如 A异常总是在B异常之后出现）

2. 根因推测（按可能性从高到低列出）
   - 每个推测附上日志中的依据

3. 排查步骤
   - 需要检查哪些指标（CPU/内存/连接池/队列堆积等）
   - 需要执行哪些命令来进一步确认

4. 应急处理
   - 如果需要立即止血，建议采取什么措施
```

### 实用排障工作流脚本

这个脚本收集服务器关键信息，喂给AI做综合分析：

```python
#!/usr/bin/env python3
"""
ai_troubleshoot.py — AI辅助排障工具
收集系统状态信息，让AI给出排障建议
"""
import subprocess
import os
from openai import OpenAI


def run_cmd(cmd: str) -> str:
    """执行命令，返回输出"""
    try:
        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=30
        )
        return result.stdout[:3000]  # 限制长度
    except Exception as e:
        return f"执行失败: {e}"


def collect_system_info() -> dict:
    """收集系统关键信息"""
    return {
        "cpu_usage": run_cmd("top -bn1 | head -20"),
        "memory": run_cmd("free -h"),
        "disk": run_cmd("df -h"),
        "network_connections": run_cmd("ss -tuln | head -30"),
        "recent_errors": run_cmd("journalctl -p err --since '30 min ago' --no-pager | tail -50"),
        "docker_status": run_cmd("docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}' 2>/dev/null || echo 'Docker not available'"),
        "top_processes": run_cmd("ps aux --sort=-%mem | head -15"),
    }


def ai_diagnose(system_info: dict, symptom: str) -> str:
    """让AI分析系统状态"""
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    info_text = "\n\n".join(
        f"=== {key} ===\n{value}" for key, value in system_info.items()
    )

    prompt = f"""你是一个资深Linux系统管理员。根据以下系统状态信息，诊断问题原因。

当前症状：{symptom}

系统状态：
{info_text}

请给出：
1. 问题诊断：根据系统状态判断最可能的问题原因
2. 关键发现：指出系统状态中哪些指标异常
3. 处理建议：具体的修复命令或操作步骤
4. 后续监控：建议关注哪些指标来防止问题复发"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=2000,
    )

    return response.choices[0].message.content


if __name__ == "__main__":
    import sys

    symptom = sys.argv[1] if len(sys.argv) > 1 else "服务响应变慢"
    print(f"正在收集系统信息...")

    info = collect_system_info()
    print(f"正在分析...")

    diagnosis = ai_diagnose(info, symptom)
    print(f"\n{'='*60}")
    print(diagnosis)
```

使用方式：

```bash
export OPENAI_API_KEY=sk-xxx
python ai_troubleshoot.py "接口P99延迟从50ms飙升到2s"
```

## AI 辅助数据库运维

### Prompt 模板：慢查询分析

```
你是一个数据库性能优化专家（{{db_type}}）。分析以下慢查询并给出优化建议。

数据库版本：{{db_version}}
数据量级：{{data_scale}} (如 用户表500万行，订单表2000万行)

慢查询SQL：
```sql
{{slow_query}}
```

执行计划（EXPLAIN输出）：
```
{{explain_output}}
```

当前相关索引：
```
{{existing_indexes}}
```

请分析：
1. 慢查询原因：为什么这条SQL慢，瓶颈在哪
2. 索引优化：是否需要添加索引，给出具体的CREATE INDEX语句
3. SQL改写：如果SQL本身可以优化，给出改写后的版本
4. 风险评估：加索引对写入性能的影响，表大小对DDL操作时间的预估
```

### 慢查询自动分析脚本

```python
#!/usr/bin/env python3
"""
slow_query_analyzer.py — 从MySQL慢查询日志提取SQL，AI分析优化
"""
import re
import os
from openai import OpenAI
import subprocess


def parse_slow_log(log_path: str, limit: int = 10) -> list[dict]:
    """解析MySQL慢查询日志，提取最近的慢查询"""
    queries = []
    current_query = []
    current_meta = {}

    with open(log_path, "r") as f:
        for line in f:
            if line.startswith("# Time:"):
                if current_query:
                    queries.append({
                        "sql": " ".join(current_query).strip(),
                        "meta": current_meta.copy(),
                    })
                    current_query = []
                    current_meta = {}
                current_meta["time"] = line.strip()
            elif line.startswith("# Query_time:"):
                match = re.search(r"Query_time:\s+([\d.]+)", line)
                if match:
                    current_meta["query_time"] = float(match.group(1))
            elif not line.startswith("#") and line.strip():
                current_query.append(line.strip())

    # 按执行时间排序，取最慢的
    queries.sort(key=lambda q: q["meta"].get("query_time", 0), reverse=True)
    return queries[:limit]


def get_explain(sql: str, db_config: dict) -> str:
    """获取SQL执行计划"""
    cmd = (
        f"mysql -h{db_config['host']} -u{db_config['user']} "
        f"-p{db_config['password']} {db_config['database']} "
        f"-e \"EXPLAIN {sql}\" 2>/dev/null"
    )
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
    return result.stdout


def analyze_with_ai(sql: str, explain_output: str, db_type: str = "MySQL 8.0") -> str:
    """AI分析慢查询"""
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    prompt = f"""你是一个{db_type}性能优化专家。分析这条慢查询。

SQL：
```sql
{sql}
```

执行计划：
```
{explain_output}
```

简洁回答：
1. 慢的原因
2. 需要加的索引（给出CREATE INDEX语句）
3. SQL改写建议（如果有的话）"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        max_tokens=1000,
    )

    return response.choices[0].message.content


# 使用
if __name__ == "__main__":
    queries = parse_slow_log("/var/log/mysql/slow.log", limit=5)
    for i, q in enumerate(queries, 1):
        print(f"\n{'='*60}")
        print(f"慢查询 #{i} (耗时: {q['meta'].get('query_time', 'N/A')}s)")
        print(f"SQL: {q['sql'][:200]}...")
        analysis = analyze_with_ai(q["sql"], "")  # 简化版，实际应传入EXPLAIN
        print(f"\nAI分析结果:\n{analysis}")
```

## 用 AI 编写运维脚本

运维脚本是AI最能发挥价值的场景之一。原因很简单：运维脚本通常逻辑不复杂但细节多、需要处理各种边界情况、写起来枯燥。

### Prompt 模板：运维脚本生成

```
帮我写一个运维脚本。

脚本功能：{{description}}
运行环境：{{os}} (如 Ubuntu 22.04 / CentOS 7)
脚本语言：{{language}} (如 Bash / Python)

具体要求：
{{requirements}}

代码质量要求：
- 加上参数校验和使用说明（--help）
- 关键操作前做检查（如磁盘空间是否够、目标服务是否在运行）
- 所有操作要有日志输出，方便排查
- 危险操作（删除、重启）要有确认提示，支持 --force 跳过
- 加上错误处理，失败时给出有意义的错误信息
- 支持 dry-run 模式，先展示要做什么但不真的执行
```

### 示例：AI生成的日志清理脚本

用上面的模板让AI生成，然后人工review过的成品：

```python
#!/usr/bin/env python3
"""
log_cleaner.py — 自动清理过期日志文件
功能：扫描指定目录，删除超过N天的日志文件，支持按大小筛选和dry-run模式
"""
import argparse
import os
import time
import logging
from pathlib import Path
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="自动清理过期日志文件")
    parser.add_argument("--dir", required=True, help="要扫描的日志目录")
    parser.add_argument("--days", type=int, default=30, help="保留天数（默认30天）")
    parser.add_argument("--pattern", default="*.log", help="文件匹配模式（默认*.log）")
    parser.add_argument("--min-size", type=int, default=0, help="最小文件大小(MB)，小于此的不清理")
    parser.add_argument("--dry-run", action="store_true", help="只展示要删除的文件，不实际执行")
    parser.add_argument("--force", action="store_true", help="跳过确认提示")
    return parser.parse_args()


def find_expired_files(
    directory: str, pattern: str, days: int, min_size_mb: int
) -> list[dict]:
    """查找过期的日志文件"""
    cutoff_time = time.time() - (days * 86400)
    expired = []

    for file_path in Path(directory).rglob(pattern):
        if not file_path.is_file():
            continue

        stat = file_path.stat()
        file_size_mb = stat.st_size / (1024 * 1024)

        if stat.st_mtime < cutoff_time and file_size_mb >= min_size_mb:
            expired.append({
                "path": str(file_path),
                "size_mb": round(file_size_mb, 2),
                "modified": datetime.fromtimestamp(stat.st_mtime).strftime(
                    "%Y-%m-%d %H:%M"
                ),
            })

    return sorted(expired, key=lambda f: f["size_mb"], reverse=True)


def main():
    args = parse_args()

    # 参数校验
    if not os.path.isdir(args.dir):
        logger.error(f"目录不存在: {args.dir}")
        return 1

    logger.info(f"扫描目录: {args.dir}")
    logger.info(f"匹配模式: {args.pattern}, 保留天数: {args.days}")

    files = find_expired_files(args.dir, args.pattern, args.days, args.min_size)

    if not files:
        logger.info("没有找到需要清理的文件")
        return 0

    total_size = sum(f["size_mb"] for f in files)
    logger.info(f"找到 {len(files)} 个过期文件，共 {total_size:.2f} MB")

    # 展示文件列表
    for f in files[:20]:  # 只展示前20个
        prefix = "[DRY-RUN] " if args.dry_run else ""
        logger.info(f"{prefix}将删除: {f['path']} ({f['size_mb']} MB, 修改于 {f['modified']})")

    if len(files) > 20:
        logger.info(f"...还有 {len(files) - 20} 个文件未列出")

    if args.dry_run:
        logger.info("[DRY-RUN] 以上文件将被删除。去掉 --dry-run 参数实际执行。")
        return 0

    # 确认
    if not args.force:
        answer = input(f"\n确认删除 {len(files)} 个文件，释放 {total_size:.2f} MB？[y/N] ")
        if answer.lower() != "y":
            logger.info("操作已取消")
            return 0

    # 执行删除
    deleted_count = 0
    deleted_size = 0
    for f in files:
        try:
            os.remove(f["path"])
            deleted_count += 1
            deleted_size += f["size_mb"]
        except Exception as e:
            logger.error(f"删除失败: {f['path']}, 错误: {e}")

    logger.info(f"清理完成: 删除 {deleted_count} 个文件，释放 {deleted_size:.2f} MB")
    return 0


if __name__ == "__main__":
    exit(main())
```

使用方式：

```bash
# 先dry-run看看会删什么
python log_cleaner.py --dir /var/log/app --days 14 --dry-run

# 确认没问题后执行
python log_cleaner.py --dir /var/log/app --days 14

# 定时任务免确认
python log_cleaner.py --dir /var/log/app --days 14 --force
```

### AI 生成运维脚本的最佳实践

1. **先让AI生成，再人工review** — AI生成的脚本80%能直接用，但那20%的边界情况可能让你半夜被叫起来。特别注意：文件路径处理（空格、特殊字符）、权限检查、并发安全。

2. **给AI看你现有的脚本风格** — 把你团队现有的一两个脚本附在Prompt里，说"遵循这个风格"。AI会自动对齐日志格式、错误处理方式、参数解析习惯。

3. **让AI写测试场景** — 脚本写完后追问："这个脚本可能在哪些场景下出问题？帮我列出需要测试的边界情况。" AI在找边界情况这件事上比大部分人强。

4. **危险操作必须有dry-run** — 这是铁律。任何涉及删除、修改、重启的脚本，都要有dry-run模式。让AI在生成时就加上这个功能。

## 小结

这一章给了你四类场景的模板和脚本：CI失败分析、运维排障、数据库优化、脚本生成。这些模板不是用来直接复制粘贴的——根据你的实际环境调整里面的参数、修改Prompt里的技术栈描述、加上你们自己的内部系统信息。

AI在DevOps领域的价值不是取代运维工程师，而是帮你从"人肉分析日志"这种低价值工作中解放出来，把时间花在架构设计、容量规划、自动化建设这些更有价值的事情上。
