# 4.9 LLM API 集成实战

这一章带你从零搭建一个AI对话服务。从最基础的API调用开始，到流式输出、Function Calling，最后搭一个多模型统一接入层。所有代码都是能直接跑的，Python 和 TypeScript 双语言。

## 基础 API 调用

### OpenAI API（Python）

先装依赖：

```bash
pip install openai
```

```python
from openai import OpenAI

client = OpenAI(api_key="sk-your-key-here")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "你是一个技术助手，回答简洁准确。"},
        {"role": "user", "content": "Python里 asyncio.gather 和 asyncio.wait 的区别是什么？"}
    ],
    temperature=0.3,
    max_tokens=1000,
)

print(response.choices[0].message.content)
```

### OpenAI API（TypeScript）

```bash
npm install openai
```

```typescript
import OpenAI from "openai";

const client = new OpenAI({ apiKey: "sk-your-key-here" });

async function chat(userMessage: string): Promise<string> {
  const response = await client.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "你是一个技术助手，回答简洁准确。" },
      { role: "user", content: userMessage },
    ],
    temperature: 0.3,
    max_tokens: 1000,
  });

  return response.choices[0].message.content ?? "";
}

// 使用
const answer = await chat("TypeScript里 type 和 interface 的区别？");
console.log(answer);
```

### Anthropic API（Python）

```bash
pip install anthropic
```

```python
from anthropic import Anthropic

client = Anthropic(api_key="sk-ant-your-key-here")

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system="你是一个技术助手，回答简洁准确。",
    messages=[
        {"role": "user", "content": "解释一下 Python 的 GIL 是什么，对多线程有什么影响。"}
    ],
)

print(response.content[0].text)
```

### Anthropic API（TypeScript）

```bash
npm install @anthropic-ai/sdk
```

```typescript
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic({ apiKey: "sk-ant-your-key-here" });

async function chat(userMessage: string): Promise<string> {
  const response = await client.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 1024,
    system: "你是一个技术助手，回答简洁准确。",
    messages: [{ role: "user", content: userMessage }],
  });

  return response.content[0].type === "text" ? response.content[0].text : "";
}
```

注意两个API的差异：OpenAI的system message放在messages数组里，Anthropic的system是独立参数。Anthropic返回的content是数组（因为可能包含多种类型），OpenAI直接是字符串。

## 流式输出（Streaming）

用户等5秒看到完整回答，和看着文字一个个蹦出来，体验完全不同。流式输出在生产环境中是必须的。

### Python 后端（FastAPI + SSE）

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI
import json

app = FastAPI()
client = OpenAI(api_key="sk-your-key-here")


async def generate_stream(user_message: str):
    """生成SSE格式的流式响应"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "你是一个技术助手。"},
            {"role": "user", "content": user_message},
        ],
        stream=True,
    )

    for chunk in stream:
        delta = chunk.choices[0].delta
        if delta.content:
            # SSE 格式: data: {json}\n\n
            data = json.dumps({"content": delta.content}, ensure_ascii=False)
            yield f"data: {data}\n\n"

    yield "data: [DONE]\n\n"


@app.post("/api/chat")
async def chat(request: dict):
    user_message = request.get("message", "")
    return StreamingResponse(
        generate_stream(user_message),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )
```

### TypeScript 前端消费流式响应

```typescript
async function streamChat(
  message: string,
  onChunk: (text: string) => void,
  onDone: () => void
) {
  const response = await fetch("/api/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ message }),
  });

  if (!response.body) throw new Error("No response body");

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let buffer = "";

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });

    // 按行解析SSE数据
    const lines = buffer.split("\n");
    buffer = lines.pop() ?? ""; // 最后一行可能不完整，留到下次处理

    for (const line of lines) {
      if (!line.startsWith("data: ")) continue;

      const data = line.slice(6).trim();
      if (data === "[DONE]") {
        onDone();
        return;
      }

      try {
        const parsed = JSON.parse(data);
        if (parsed.content) {
          onChunk(parsed.content);
        }
      } catch {
        // 忽略解析失败的行
      }
    }
  }
}

// React 组件中使用
function ChatComponent() {
  const [reply, setReply] = useState("");

  const handleSend = async (message: string) => {
    setReply("");
    await streamChat(
      message,
      (chunk) => setReply((prev) => prev + chunk),
      () => console.log("Stream finished")
    );
  };
  // ...
}
```

## Function Calling / Tool Use 实战

Function Calling 让LLM能调用你定义的函数。模型不直接执行函数——它只是告诉你"我想调用这个函数，参数是这些"，你负责执行并把结果返回给它。

### 完整示例：带天气查询和数据库查询的助手（Python）

```python
from openai import OpenAI
import json

client = OpenAI(api_key="sk-your-key-here")

# 第一步：定义工具
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "获取指定城市的当前天气信息",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "城市名称，如：北京、上海",
                    }
                },
                "required": ["city"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "query_order",
            "description": "根据订单号查询订单状态",
            "parameters": {
                "type": "object",
                "properties": {
                    "order_id": {
                        "type": "string",
                        "description": "订单号",
                    }
                },
                "required": ["order_id"],
            },
        },
    },
]


# 第二步：实现工具函数（这里用模拟数据）
def get_weather(city: str) -> dict:
    # 实际项目中调用天气API
    mock_data = {
        "北京": {"temp": 22, "condition": "晴", "humidity": 45},
        "上海": {"temp": 26, "condition": "多云", "humidity": 72},
    }
    return mock_data.get(city, {"error": f"未找到{city}的天气数据"})


def query_order(order_id: str) -> dict:
    # 实际项目中查数据库
    return {"order_id": order_id, "status": "已发货", "estimated_delivery": "2025-03-15"}


# 工具函数映射
tool_handlers = {
    "get_weather": get_weather,
    "query_order": query_order,
}


# 第三步：对话循环，处理工具调用
def chat_with_tools(user_message: str) -> str:
    messages = [
        {"role": "system", "content": "你是一个客服助手，可以查询天气和订单信息。"},
        {"role": "user", "content": user_message},
    ]

    while True:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            tools=tools,
        )

        choice = response.choices[0]

        # 如果模型不需要调用工具，直接返回文本回复
        if choice.finish_reason == "stop":
            return choice.message.content

        # 如果模型要调用工具
        if choice.finish_reason == "tool_calls":
            # 先把模型的回复（含工具调用请求）加入消息历史
            messages.append(choice.message)

            # 执行每个工具调用
            for tool_call in choice.message.tool_calls:
                func_name = tool_call.function.name
                func_args = json.loads(tool_call.function.arguments)

                # 调用对应的处理函数
                handler = tool_handlers.get(func_name)
                if handler:
                    result = handler(**func_args)
                else:
                    result = {"error": f"未知工具: {func_name}"}

                # 把工具执行结果返回给模型
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result, ensure_ascii=False),
                })

            # 继续循环，让模型根据工具结果生成最终回复


# 使用
answer = chat_with_tools("帮我查一下北京天气，顺便看看订单 ORD-20250301 到哪了")
print(answer)
```

关键点：模型可能一次请求调用多个工具（比如同时查天气和查订单），你需要把所有工具的结果都返回，然后让模型基于所有结果生成最终回答。这是一个循环过程，直到模型不再请求工具调用为止。

## 多模型统一接入层

实际项目中你大概率需要同时用多个模型——GPT-4o处理通用任务，Claude处理长文本，某个国产模型处理中文场景。写一个适配器统一接入，别让业务代码跟具体模型耦合。

### TypeScript 实现

```typescript
// types.ts
interface ChatMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
}

interface ChatResponse {
  content: string;
  model: string;
  usage: {
    inputTokens: number;
    outputTokens: number;
  };
}

// 适配器接口
interface LLMProvider {
  chat(request: ChatRequest): Promise<ChatResponse>;
  chatStream(
    request: ChatRequest,
    onChunk: (text: string) => void
  ): Promise<void>;
}
```

```typescript
// providers/openai-provider.ts
import OpenAI from "openai";

class OpenAIProvider implements LLMProvider {
  private client: OpenAI;
  private model: string;

  constructor(apiKey: string, model = "gpt-4o") {
    this.client = new OpenAI({ apiKey });
    this.model = model;
  }

  async chat(request: ChatRequest): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: this.model,
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
    });

    return {
      content: response.choices[0].message.content ?? "",
      model: this.model,
      usage: {
        inputTokens: response.usage?.prompt_tokens ?? 0,
        outputTokens: response.usage?.completion_tokens ?? 0,
      },
    };
  }

  async chatStream(
    request: ChatRequest,
    onChunk: (text: string) => void
  ): Promise<void> {
    const stream = await this.client.chat.completions.create({
      model: this.model,
      messages: request.messages,
      temperature: request.temperature ?? 0.7,
      max_tokens: request.maxTokens ?? 2000,
      stream: true,
    });

    for await (const chunk of stream) {
      const text = chunk.choices[0]?.delta?.content;
      if (text) onChunk(text);
    }
  }
}
```

```typescript
// providers/anthropic-provider.ts
import Anthropic from "@anthropic-ai/sdk";

class AnthropicProvider implements LLMProvider {
  private client: Anthropic;
  private model: string;

  constructor(apiKey: string, model = "claude-sonnet-4-20250514") {
    this.client = new Anthropic({ apiKey });
    this.model = model;
  }

  async chat(request: ChatRequest): Promise<ChatResponse> {
    // 拆分 system message（Anthropic 需要独立传入）
    const systemMsg = request.messages.find((m) => m.role === "system");
    const otherMsgs = request.messages.filter((m) => m.role !== "system");

    const response = await this.client.messages.create({
      model: this.model,
      max_tokens: request.maxTokens ?? 2000,
      system: systemMsg?.content ?? "",
      messages: otherMsgs.map((m) => ({
        role: m.role as "user" | "assistant",
        content: m.content,
      })),
    });

    return {
      content:
        response.content[0].type === "text" ? response.content[0].text : "",
      model: this.model,
      usage: {
        inputTokens: response.usage.input_tokens,
        outputTokens: response.usage.output_tokens,
      },
    };
  }

  async chatStream(
    request: ChatRequest,
    onChunk: (text: string) => void
  ): Promise<void> {
    const systemMsg = request.messages.find((m) => m.role === "system");
    const otherMsgs = request.messages.filter((m) => m.role !== "system");

    const stream = this.client.messages.stream({
      model: this.model,
      max_tokens: request.maxTokens ?? 2000,
      system: systemMsg?.content ?? "",
      messages: otherMsgs.map((m) => ({
        role: m.role as "user" | "assistant",
        content: m.content,
      })),
    });

    for await (const event of stream) {
      if (
        event.type === "content_block_delta" &&
        event.delta.type === "text_delta"
      ) {
        onChunk(event.delta.text);
      }
    }
  }
}
```

```typescript
// llm-client.ts — 统一入口
class LLMClient {
  private providers = new Map<string, LLMProvider>();
  private defaultProvider: string;

  constructor(defaultProvider: string) {
    this.defaultProvider = defaultProvider;
  }

  register(name: string, provider: LLMProvider) {
    this.providers.set(name, provider);
  }

  private getProvider(name?: string): LLMProvider {
    const providerName = name ?? this.defaultProvider;
    const provider = this.providers.get(providerName);
    if (!provider) throw new Error(`Provider "${providerName}" not registered`);
    return provider;
  }

  async chat(request: ChatRequest, provider?: string): Promise<ChatResponse> {
    return this.getProvider(provider).chat(request);
  }

  async chatStream(
    request: ChatRequest,
    onChunk: (text: string) => void,
    provider?: string
  ): Promise<void> {
    return this.getProvider(provider).chatStream(request, onChunk);
  }
}

// 初始化
const llm = new LLMClient("openai");
llm.register("openai", new OpenAIProvider(process.env.OPENAI_API_KEY!));
llm.register(
  "anthropic",
  new AnthropicProvider(process.env.ANTHROPIC_API_KEY!)
);

// 业务代码中使用——切换模型只需改一个参数
const result = await llm.chat(
  {
    messages: [
      { role: "system", content: "你是技术助手。" },
      { role: "user", content: "解释一下微服务架构的优缺点" },
    ],
  },
  "anthropic" // 不传就用默认的 openai
);
```

这个设计的好处：业务代码完全不感知具体用的是哪个模型，切换模型只需要改调用时的provider参数。后面要加新模型（比如接入DeepSeek），只需要实现一个新的Provider类并注册即可，业务代码一行不用动。

## 生产环境注意事项

几个容易踩的坑提前说：

1. **API Key 不要硬编码**，用环境变量或密钥管理服务
2. **加上重试机制**，API偶尔会超时或返回500，用指数退避重试
3. **设置合理的超时时间**，流式请求的超时要比普通请求长
4. **记录token用量**，每次调用把usage信息存下来，方便监控成本
5. **做好降级策略**，主模型不可用时自动切换到备用模型
