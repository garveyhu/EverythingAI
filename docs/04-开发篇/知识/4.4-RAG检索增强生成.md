# 4.4 RAG检索增强生成

你用ChatGPT的时候一定遇到过这种情况：问它一个公司内部的流程问题，它要么说"我不清楚"，要么一本正经地编一个答案。这不是模型笨，是因为它根本没见过你公司的数据。

RAG（Retrieval-Augmented Generation，检索增强生成）就是解决这个问题的。它的思路非常直觉：既然模型不知道，那我先帮它从知识库里找到相关的资料，塞到prompt里一起喂给它，让它基于这些资料来回答。

## 为什么需要RAG

LLM有三个硬伤直接催生了RAG的需求。

**知识截止问题。** 模型的训练数据有截止日期，它不知道上个月发布的新版本文档里写了什么。

**幻觉问题。** 模型会在不确定的时候"编"答案，而且编得很有自信。对于企业应用来说，这是不可接受的——客服机器人如果胡说八道，比不回答更糟糕。

**私有数据问题。** 你的产品文档、内部Wiki、客户合同，这些数据不在模型的训练集里，模型对它们一无所知。你也不可能为了让模型了解这些数据就去微调模型——成本高、周期长、数据更新了还得重新调。

RAG的优势在于：不需要改模型，只需要在调用模型之前多一步"检索"，就能让模型拥有你的私有知识。而且数据更新了，只需要更新知识库，不需要重新训练模型。

## RAG架构全景

一个标准的RAG流程分四步：

```
用户提问 → 检索相关文档片段 → 把文档片段塞进prompt → 模型基于文档生成回答
```

展开来看，完整架构包含两条线：

**离线索引线（数据准备阶段）：** 把原始文档（PDF、网页、数据库记录）拆分成小块（chunking），用嵌入模型把每个块转成向量，存入向量数据库。这一步只需要做一次（数据更新时增量更新）。

**在线查询线（用户使用阶段）：** 用户提问 → 把问题转成向量 → 在向量数据库中搜索最相似的文档块 → 把搜到的文档块和用户问题一起发给LLM → LLM基于提供的材料生成回答。

看起来不复杂，但每一步都有学问。魔鬼在细节里。

## 文本分割策略

文本分割是RAG效果的第一道关卡。切太大，检索精度低（一大段文本里可能只有一句话是相关的）；切太小，缺乏上下文，模型拿到一个断章取义的片段也生成不出好的回答。

**固定大小分割**是最简单的方式。比如每500个字符切一块，块与块之间留100个字符的重叠。实现简单，但它不管语义——可能把一句话从中间切断。

**递归分割**是更聪明的方式，也是LangChain推荐的默认策略。它按照一个分隔符优先级列表来切：先按章节标题切，切完如果块还是太大，再按段落切，还大就按句子切。这样能在尽量保持语义完整性的前提下控制块的大小。

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # 每块最大500字符
    chunk_overlap=100,     # 块之间重叠100字符
    separators=["\n\n", "\n", "。", "，", " "]  # 分隔符优先级
)
chunks = splitter.split_text(document_text)
```

**语义分割**更高级，它用嵌入模型判断相邻句子的语义相似度，当相似度突然下降时，说明话题变了，就在那里切。效果最好但也最慢。

我的建议是：**先用递归分割跑通流程，chunk_size设400-600之间，overlap设10%-20%。** 等到发现检索效果不行时，再去调分割策略。不要一上来就追求完美。

## 嵌入模型

嵌入模型（Embedding Model）是把文本转成向量的工具。它的作用是让计算机能够"度量"两段文本的语义相似度。"如何重置密码"和"忘记密码怎么办"在字面上很不同，但嵌入向量会很接近。

几个主流选择：

**OpenAI text-embedding-3-small/large。** 使用最广泛，效果好，接入简单，缺点是要走外网、有成本。

**Cohere embed-v3。** 在多语言场景下表现很强，特别是非英语文本。

**BGE系列（智源）。** 开源模型里的标杆，中文效果好，可以本地部署，零API成本。如果你的数据主要是中文且不想依赖外部API，BGE是首选。

**选型原则：** 不要只看公开榜单排名。拿你自己的真实数据测。准备100个问答对，跑一遍检索看召回率。有时候一个在榜单上排名靠后的模型，在你的特定领域反而效果更好。

另一个容易踩的坑：**索引时和查询时必须用同一个嵌入模型。** 不同模型生成的向量是不同维度、不同空间的，混用等于废了。

## 向量数据库选型

向量数据库存储嵌入向量并提供高效的相似度搜索。选型取决于你的规模和技术栈。

**Chroma。** 嵌入式向量数据库，pip install就能用，数据存本地文件。适合原型验证和小规模应用。但不适合生产环境的多实例部署。

**Pgvector。** PostgreSQL的向量搜索扩展。如果你的项目已经在用PostgreSQL，加个扩展就能存向量，不需要额外维护一个数据库。百万级数据量以内性能够用。**这是我最推荐的起步方案——不要为了存向量单独引入一个新的数据库**。

**Milvus。** 专门的向量数据库，性能强，支持数十亿级别的向量检索。适合数据量大、对检索性能要求高的场景。代价是部署和运维相对复杂。

**Pinecone。** 全托管的云服务，不需要你操心部署运维。按量付费。适合不想碰基础设施、团队没有运维能力的场景。

## 检索策略

把向量存进去只是第一步，怎么检索决定了RAG的效果上限。

**基础的语义检索**就是计算用户问题向量和库中所有文档块向量的余弦相似度，取topK。大部分场景下这就够了。

**混合检索（Hybrid Search）**把语义检索和传统的关键词检索结合起来。语义检索擅长理解意图（"怎么退货" vs "退货流程"），关键词检索擅长精确匹配（产品型号、专有名词）。两者结合通常比单用任何一种效果都好。

```python
# 伪代码：混合检索
semantic_results = vector_db.similarity_search(query, k=10)
keyword_results = elasticsearch.search(query, k=10)

# 用RRF(Reciprocal Rank Fusion)合并排序
final_results = reciprocal_rank_fusion(semantic_results, keyword_results)
top_chunks = final_results[:5]
```

**重排序（Reranking）**是在初步检索之后加的一步精排。先用向量检索召回20-50个候选，再用一个Cross-Encoder模型对"问题-文档块"的相关性做精确打分，取分数最高的几个。Cohere的Rerank API和开源的bge-reranker都能做这件事。加上reranking通常能让最终效果提升一个台阶。

## RAG优化方向

当你的基础RAG跑通但效果不够好时，可以从以下几个方向优化。

**查询改写。** 用户的提问经常很模糊或者很口语化。在检索之前让LLM把用户问题改写成更适合检索的形式，甚至拆成多个子问题分别检索，效果会好很多。

**上下文窗口管理。** 检索出来的文档块塞进prompt，要注意不要超过模型的上下文窗口。同时也要注意，塞太多噪音文档反而会干扰模型判断。一般3-5个最相关的块就够了。

**来源追溯。** 在prompt里要求模型标注回答基于哪些文档片段，给用户展示出处。这既增加可信度，也方便排查错误。

## RAG vs 微调：怎么选

这个问题经常被问到。简单的判断标准：

**用RAG：** 知识会频繁更新（产品文档、FAQ）、需要明确的来源引用、数据敏感不想用于模型训练、要快速上线。

**用微调：** 要改变模型的风格或行为模式（让它模仿特定语气）、任务很固定且需要极高的准确率、想减少prompt长度降低推理成本。

大多数企业场景，**先做RAG**。微调的成本和复杂度比RAG高一个数量级，而RAG在大部分知识问答场景下效果已经足够好。
