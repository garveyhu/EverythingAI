# 4.5 AI Agent开发

前面讲的RAG本质上还是"你问我答"——用户发一个问题，系统检索一下，LLM生成一个回复，结束。Agent要做的事情不一样：**它不只是回答问题，它能自主完成任务。**

你跟Agent说"帮我查一下上周的销售数据，生成一份分析报告，发到我邮箱"，它会自己拆解步骤：先调用数据库查询接口拿数据，再用分析能力生成报告，最后调用邮件接口发送。整个过程中你不需要逐步指挥它。

## Agent的核心：推理 + 行动 的循环

一个最基础的Agent运行流程是这样的：

```
用户给一个任务
→ LLM思考：要完成这个任务，下一步该做什么？
→ LLM决定：调用某个工具（比如搜索API）
→ 系统执行工具，拿到结果
→ LLM观察结果：还需要做什么？
→ 继续思考、行动、观察...
→ 直到任务完成，给出最终回答
```

这个循环就是ReAct模式的核心。理解了它，你就理解了大部分Agent框架在做什么。

## 三种主流Agent模式

### ReAct（推理+行动）

ReAct是最基础也最常用的Agent模式。模型在每一步都会先输出"思考过程"（Thought），然后决定一个"动作"（Action），执行后得到"观察"（Observation），再进入下一轮。

```
用户：杭州明天适合户外活动吗？

Thought: 我需要查询杭州明天的天气信息
Action: get_weather(city="杭州", date="明天")
Observation: 杭州明天晴，气温18-26℃，空气质量良

Thought: 天气不错，我还需要看看是否有紫外线过强的问题
Action: get_uv_index(city="杭州", date="明天")
Observation: 紫外线指数：中等

Thought: 信息够了，可以给出建议了
Answer: 杭州明天天气晴朗，气温18-26℃，比较舒适。紫外线中等，
建议做好防晒。整体很适合户外活动。
```

ReAct的优势是简单直观，每一步都可追踪。缺点是遇到复杂任务时，模型可能在循环里打转，或者走偏方向。

### Plan-and-Execute（规划+执行）

对于复杂任务，先让LLM制定一个完整计划，然后逐步执行，比ReAct的"走一步看一步"要靠谱。

```
用户：帮我做一份竞品分析报告

Plan:
1. 确定竞品列表（调用搜索工具）
2. 收集每个竞品的产品功能信息
3. 收集每个竞品的定价策略
4. 整理对比表格
5. 生成分析报告

Execute Step 1: search("XX行业主要竞品")
Execute Step 2: search("竞品A 产品功能") ...
...
```

这种模式下，规划和执行可以用不同的模型：规划用能力强的大模型（比如Claude Sonnet或GPT-4o），执行步骤中简单的任务用便宜的小模型，能节省不少成本。

### Multi-Agent（多智能体协作）

当任务足够复杂时，可以拆分给多个专门的Agent合作完成。每个Agent有自己的角色、工具和能力范围。

比如做一个自动化的代码开发流程：

```
产品Agent：负责理解需求，输出技术规格
架构Agent：基于规格设计方案
编码Agent：按照方案写代码
测试Agent：为代码编写并执行测试
审查Agent：review代码质量
```

Multi-Agent的难点在于Agent之间的协调——信息怎么传递、冲突怎么解决、一个Agent的错误怎么不影响整条链路。

不过这个方向在2026年已经有了非常实际的落地。**Claude Code的Agent Teams（蜂群模式）** 就是Multi-Agent在代码开发场景的真实应用：一个Team Lead实例负责理解需求和分解任务，然后创建多个Teammate实例并行工作——前端Agent、后端Agent、测试Agent各司其职，每个都在独立的Git Worktree中操作以避免文件冲突，最后由Team Lead合并成果。这不是Demo，Anthropic自己的安全团队用16个并行Agent在两周内写了10万行Rust代码的C编译器。Google Antigravity的Manager View也是类似思路，支持同时调度多个Agent并行处理不同任务。

## 工具调用：Agent的"手脚"

LLM只能生成文本，它不能真的去查数据库、发邮件、调API。工具调用机制（Function Calling / Tool Use）是让Agent拥有行动能力的关键。

原理是这样的：你在调用LLM时，把可用的工具列表和每个工具的参数定义传给模型。模型在需要的时候，不会直接回答，而是返回一个"我想调用这个工具"的结构化指令。你的代码执行这个工具，把结果返回给模型，模型再继续。

```python
import json
from openai import OpenAI

client = OpenAI()

# 定义工具
tools = [
    {
        "type": "function",
        "function": {
            "name": "query_database",
            "description": "查询销售数据库",
            "parameters": {
                "type": "object",
                "properties": {
                    "sql": {"type": "string", "description": "SQL查询语句"},
                },
                "required": ["sql"]
            }
        }
    }
]

# 第一轮：模型决定调用工具
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "上周销售额最高的产品是什么？"}],
    tools=tools
)

# 模型返回的不是文字，而是工具调用请求
tool_call = response.choices[0].message.tool_calls[0]
function_name = tool_call.function.name
arguments = json.loads(tool_call.function.arguments)
# arguments = {"sql": "SELECT product_name, SUM(amount) ..."}

# 你的代码执行查询
result = execute_sql(arguments["sql"])

# 第二轮：把工具结果传回模型
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "user", "content": "上周销售额最高的产品是什么？"},
        response.choices[0].message,  # 包含tool_calls的消息
        {"role": "tool", "tool_call_id": tool_call.id, "content": str(result)}
    ],
    tools=tools
)
# 模型基于查询结果生成自然语言回答
print(response.choices[0].message.content)
```

工具定义的质量直接影响Agent的行为。description要写清楚工具做什么、什么时候该用；参数描述要精确，这些都是模型判断"该不该调这个工具"的依据。

## Agent开发框架

自己从零写Agent当然可以，但框架能帮你省掉大量胶水代码。

**LangChain / LangGraph。** LangChain是目前生态最大的LLM应用框架，但说实话它的抽象层次偏重，简单的事情搞复杂了。LangGraph是它的新一代方案，用图结构定义Agent的工作流，可控性比LangChain好很多。如果你要做有复杂状态流转的Agent，LangGraph值得看。但如果你的需求简单，直接用模型API + 几十行Python代码可能比引入LangChain更清爽。

**CrewAI。** 专门做多Agent协作的框架。你定义角色、分配任务、设定协作方式，框架帮你管理Agent之间的交互。API设计比较友好，适合快速搭建多Agent原型。

**AutoGen（微软）。** 也是做多Agent的，设计思路是把Agent之间的交互建模成"对话"。它在自动化代码生成和执行这个方向上做得比较深。

**Dify / Coze。** 低代码平台，通过拖拽方式构建Agent工作流。不需要写多少代码，适合快速验证想法或者让非开发人员搭建简单的Agent。但灵活性和可定制性比代码方案差很多。如果你的Agent需要复杂的业务逻辑，最终还是得回到代码。

**Claude Code Agent Teams。** 如果你的场景是代码开发，Claude Code的Agent Teams（蜂群模式）是目前最成熟的Multi-Agent实现。它不是一个需要你编码的框架，而是一个开箱即用的产品——启用后在终端里告诉Team Lead你想要什么，它自动创建Teammate并行工作，每个在独立Git Worktree中操作。4.11实操篇会详细介绍怎么用。

**我的建议：** 先不要急着选框架。用原生API手写一个简单的ReAct Agent，理解底层原理之后，再根据你的具体需求选框架。很多人直接上LangChain，遇到问题调试起来一头雾水，因为不理解底下到底发生了什么。如果你只是想在开发流程中用Multi-Agent，直接用Claude Code Agent Teams就行，不需要自己造轮子。

## Agent的记忆系统

Agent需要记住之前发生过什么，不然每次交互都是从零开始。

**短期记忆**就是当前对话的消息列表。最简单的实现就是把所有消息都传给模型，但消息多了会超出上下文窗口。你需要做截断或者摘要——比如只保留最近20轮对话，更早的内容让LLM总结成一段摘要放在前面。

**长期记忆**是跨会话的信息持久化。比如用户之前说过"我是Python开发者"，下次对话时Agent应该记得这个偏好。实现方式通常是把关键信息提取出来存到数据库（向量数据库或关系型数据库），下次对话时检索相关记忆注入上下文。

```python
# 简化的记忆管理逻辑
class AgentMemory:
    def __init__(self):
        self.messages = []         # 短期记忆：当前对话
        self.summary = ""          # 对话摘要
        self.user_profile = {}     # 长期记忆：用户画像

    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
        # 消息超过阈值时做摘要压缩
        if len(self.messages) > 30:
            self.summary = self.summarize(self.messages[:20])
            self.messages = self.messages[20:]

    def get_context(self):
        context = []
        if self.summary:
            context.append({"role": "system", "content": f"之前的对话摘要：{self.summary}"})
        context.extend(self.messages)
        return context
```

## Agent安全性：不能让Agent失控

Agent能调用工具、操作外部系统，这意味着它有可能造成真实的损害。安全设计必须从一开始就考虑。

**权限最小化。** Agent能调用的工具要严格限定。如果它只需要查询数据库，就不要给它写入权限。不要因为"万一将来需要"就开放不必要的权限。

**操作边界。** 高危操作（删除数据、发送邮件、修改配置）必须有人工确认环节。可以在工具层面做拦截：

```python
def execute_tool(tool_name, args):
    if tool_name in HIGH_RISK_TOOLS:
        # 高危操作需要人工确认
        approved = request_human_approval(tool_name, args)
        if not approved:
            return "操作被用户拒绝"
    return tools[tool_name](**args)
```

**循环检测。** Agent可能在错误的循环里打转，不断重复相同的失败操作。设置最大步数限制，超过就强制终止并告警。

**输入输出过滤。** 用户可能通过精心构造的输入，让Agent调用不该调用的工具（prompt注入的一种形式）。在工具调用前要做参数校验，在结果返回前要做内容过滤。这个话题在4.7节安全篇会详细展开。

Agent是当前AI应用里最令人兴奋也最有挑战的方向。它把LLM从一个"问答机器"变成了一个"任务执行器"，想象空间巨大。但同时也意味着更高的工程复杂度和更大的安全风险。循序渐进，先做简单的再做复杂的。
