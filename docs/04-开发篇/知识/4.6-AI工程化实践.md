# 4.6 AI工程化实践

前面几节讲的是怎么让AI功能"跑起来"。这节讲的是怎么让它"跑得稳、跑得省、跑得好维护"。从Demo到生产，中间隔着一道叫"工程化"的鸿沟。

## AI服务层抽象

你的业务代码不应该直接调用OpenAI的SDK。原因很简单：模型会换，供应商会换，API会升级。如果你的代码里到处散布着`openai.chat.completions.create()`，换一个模型就要改几十个文件。

正确的做法是在业务代码和模型API之间加一层抽象：

```python
# ai_service.py - AI服务抽象层
from abc import ABC, abstractmethod

class LLMService(ABC):
    @abstractmethod
    async def chat(self, messages: list, **kwargs) -> str:
        pass

    @abstractmethod
    async def chat_stream(self, messages: list, **kwargs):
        pass

class OpenAIService(LLMService):
    async def chat(self, messages, **kwargs):
        response = await self.client.chat.completions.create(
            model=kwargs.get("model", "gpt-4o"),
            messages=messages,
            temperature=kwargs.get("temperature", 0.7)
        )
        return response.choices[0].message.content

class AnthropicService(LLMService):
    async def chat(self, messages, **kwargs):
        # Anthropic的API格式转换
        response = await self.client.messages.create(
            model=kwargs.get("model", "claude-sonnet-4-20250514"),
            messages=messages,
            max_tokens=kwargs.get("max_tokens", 2048)
        )
        return response.content[0].text

# 业务代码只依赖抽象接口
class DocumentSummarizer:
    def __init__(self, llm: LLMService):
        self.llm = llm  # 注入哪个实现，就用哪个模型

    async def summarize(self, text: str) -> str:
        messages = [
            {"role": "system", "content": "你是一个文档摘要助手"},
            {"role": "user", "content": f"请总结以下文档：\n{text}"}
        ]
        return await self.llm.chat(messages)
```

这样做的好处不只是换模型方便。你还可以在这一层统一做日志记录、成本统计、重试逻辑、限流控制，业务代码完全不用操心这些。

## 异步处理与队列

LLM的响应时间通常在2-30秒，这对于同步的Web请求来说太慢了。用户发起一个AI任务，你不能让他干等着。

对于实时性要求高的场景（聊天、代码补全），用**流式响应（SSE / WebSocket）**，让前端逐步接收内容。

对于非实时场景（生成报告、批量处理），用**异步任务队列**。用户提交任务后立即返回一个taskId，后台Worker从队列中取任务处理，完成后通知前端。

```python
# 使用Celery做异步任务的简化示例
from celery import Celery

app = Celery('ai_tasks', broker='redis://localhost:6379')

@app.task(bind=True, max_retries=3)
def generate_report(self, user_id, report_params):
    try:
        llm = get_llm_service()
        result = llm.chat(build_report_prompt(report_params))
        save_report(user_id, result)
        notify_user(user_id, "报告生成完成")
    except Exception as e:
        # 重试，间隔指数增长
        self.retry(exc=e, countdown=2 ** self.request.retries)
```

## 缓存策略

同一个问题被问了一百次，你没必要调一百次API。缓存能帮你省钱又提速。

**精确缓存**是最简单的：对prompt做hash，相同的prompt直接返回缓存结果。适合prompt完全固定的场景（比如对同一份文档做摘要）。

**语义缓存**更智能：不是看prompt是否完全相同，而是看语义是否相似。"怎么重置密码"和"我忘记密码了怎么办"，语义缓存可以识别为同一个问题，复用之前的回答。实现方式是把问题做embedding，搜索向量数据库中是否有足够相似的已有问答对。

```python
class SemanticCache:
    def __init__(self, similarity_threshold=0.92):
        self.threshold = similarity_threshold
        self.vector_store = get_vector_store()

    async def get(self, query: str):
        query_embedding = await embed(query)
        results = self.vector_store.search(query_embedding, top_k=1)
        if results and results[0].score > self.threshold:
            return results[0].metadata["answer"]
        return None

    async def set(self, query: str, answer: str):
        query_embedding = await embed(query)
        self.vector_store.insert(
            embedding=query_embedding,
            metadata={"query": query, "answer": answer}
        )
```

语义缓存的阈值需要仔细调。设太低会导致不相关的问题复用错误答案，设太高跟没缓存差不多。从0.92开始试，根据实际效果调整。

## 降级与熔断

AI服务跟其他外部服务一样，会挂、会慢、会抽风。你的系统必须能在AI服务不可用时继续运转。

**降级策略：** 当主模型不可用时，自动切换到备选模型；当所有模型都不可用时，返回预设的兜底回复或者直接告诉用户"AI功能暂时不可用"。

**熔断机制：** 当错误率超过阈值（比如连续5次调用失败），自动停止调用AI服务一段时间，避免无意义的重试浪费资源。隔一段时间后放几个请求试探，恢复了再打开。

```python
class AIServiceBreaker:
    def __init__(self):
        self.failure_count = 0
        self.state = "closed"  # closed正常, open熔断, half_open试探
        self.last_failure_time = None

    async def call(self, func, *args, **kwargs):
        if self.state == "open":
            if time.time() - self.last_failure_time > 60:  # 60秒后试探
                self.state = "half_open"
            else:
                return self.fallback()  # 返回兜底方案

        try:
            result = await func(*args, **kwargs)
            self.failure_count = 0
            self.state = "closed"
            return result
        except Exception:
            self.failure_count += 1
            self.last_failure_time = time.time()
            if self.failure_count >= 5:
                self.state = "open"
            return self.fallback()

    def fallback(self):
        return "AI服务暂时不可用，请稍后再试"
```

## 模型选型与成本优化

不是所有任务都需要GPT-4o或Claude Sonnet。模型选型的核心原则是：**用刚好够用的模型**。

一个实际的分级策略：

| 任务类型 | 推荐模型 | 理由 |
|---------|---------|------|
| 文本分类、情感分析 | GPT-4o-mini / Claude Haiku | 简单任务，小模型足够 |
| 通用对话、客服问答 | GPT-4o / Claude Sonnet | 平衡性价比和质量 |
| 复杂推理、代码生成 | Claude Sonnet / GPT-4o | 需要强推理能力 |
| 长文档理解 | Claude Sonnet（200K上下文） | 上下文窗口够大 |

**Prompt优化降低Token消耗**也是实打实的省钱手段。几个具体技巧：

把冗余的system prompt精简，只保留必要的指令。一个500 token的system prompt如果能精简到200 token，每次调用省300 token，一百万次调用就是三亿token。

对于重复性任务，用few-shot examples代替长篇说明。三个好的示例往往比一段500字的描述更有效，token也可能更少。

输出格式用JSON而不是自然语言，既省token又方便程序解析。

**本地模型部署**是另一条降成本的路。用Ollama可以在你的服务器（甚至开发机器）上跑开源模型：

```bash
# 安装Ollama并运行Qwen模型
ollama pull qwen2.5:7b
ollama serve
# 然后就可以通过兼容OpenAI的API访问
# http://localhost:11434/v1/chat/completions
```

本地模型适合对延迟敏感、数据不能出内网、调用量大到API成本不划算的场景。代价是你得有GPU服务器，而且开源模型的效果跟闭源顶级模型还是有差距。

## 监控与可观测性

AI功能上线之后，你需要知道三件事：它好不好用、它花了多少钱、它有没有在出问题。

**Prompt/Response日志。** 每次AI调用都要记录完整的输入输出，包括使用的模型、token数量、响应时间、是否命中缓存。这些日志既是排查问题的基础，也是后续优化prompt的数据来源。注意脱敏——如果用户输入包含敏感信息，日志里要做处理。

```python
async def call_llm_with_logging(messages, **kwargs):
    start_time = time.time()
    try:
        response = await llm.chat(messages, **kwargs)
        log_ai_call(
            model=kwargs.get("model"),
            input_tokens=response.usage.prompt_tokens,
            output_tokens=response.usage.completion_tokens,
            latency=time.time() - start_time,
            status="success",
            # 注意：生产环境考虑是否记录完整prompt/response
        )
        return response
    except Exception as e:
        log_ai_call(status="error", error=str(e), ...)
        raise
```

**质量指标。** 技术指标（成功率、延迟、token量）容易采集，但更重要的是业务质量指标——AI回答得好不好。最简单的方式是在前端加个"有用/没用"的反馈按钮，收集用户评价。更系统的做法是定期抽样做人工评估，或者用另一个LLM对AI输出做自动化质量打分。

**成本监控。** 按模型、按业务线、按功能模块统计每日/每月的API调用费用。设置预算阈值和预警，避免某个bug导致无限循环调用API、一夜之间烧掉一个月的预算。这种事真的发生过，不只一次。
