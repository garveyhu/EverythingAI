# 5.3 AI测试的质量保障

用AI生成测试用例确实快，但快不等于好。我见过有些团队刚开始用AI辅助测试的时候特别兴奋——"一分钟就出了50条用例！"——但仔细一看，里面有重复的、有不符合实际业务逻辑的、有前置条件写错的。如果不加评审直接用，质量反而可能下降。

AI辅助测试的核心矛盾在于：它帮你提高了效率，但同时也引入了新的质量风险。这一节我们专门聊怎么管控这些风险。

## AI生成测试用例的评审标准

对AI生成的测试用例，我建议建立一套明确的评审检查项。不需要搞得很复杂，但几个关键维度必须过一遍。

**业务准确性。** AI不了解你们公司的业务上下文。它生成的用例在"通用逻辑"层面通常没问题，但在"业务特殊逻辑"层面经常出错。比如你们的电商系统规定"会员折扣和满减优惠不能同时使用"，AI如果不知道这条规则，就可能生成一个同时测试两种优惠叠加的用例，看起来合理但实际上测的是一个不存在的场景。所以第一步永远是：拿着AI的输出对照业务规则逐条核实。

**可执行性。** AI生成的步骤有时候看起来逻辑通顺，但实际执行不了。比如它可能写"在管理后台修改用户等级为VIP"，但你们系统里用户等级是根据消费金额自动计算的，没有手动修改入口。这类问题AI不可能知道，需要你用对系统的了解来过滤。

**覆盖完整性。** AI生成的用例容易出现两种偏差：要么是某个方向过度展开（比如生成了十几条输入校验的用例），要么是某个重要维度完全漏掉（比如忘了测权限控制）。评审时建议先从宏观角度看覆盖面——功能点有没有遗漏、非功能性需求有没有考虑、异常场景覆盖够不够——然后再看具体用例的质量。

**去重和合并。** AI有时候会用不同的措辞描述本质上相同的测试场景。特别是当你分多次让它生成用例的时候，重复率会更高。评审时需要做一次去重，把重叠的用例合并。

## AI辅助测试的覆盖率评估

传统的测试覆盖率评估方法（需求覆盖率、代码覆盖率、路径覆盖率）在引入AI后依然适用，但有一些新的考量。

一个实用的做法是让AI本身来帮你做覆盖率分析。你把需求文档和已有的测试用例同时给AI，让它对比分析："这些测试用例是否完整覆盖了需求中的所有功能点和异常场景？有哪些遗漏？"AI在做这种"对照检查"的任务时表现相当不错，因为它本质上就是在做文本匹配和逻辑推理。

但要注意，AI的覆盖率分析也有盲区。它只能基于你给它的需求文档来判断覆盖情况，如果需求文档本身就不完整（这在实际项目中太常见了），AI的分析结果就会有偏差。所以AI的覆盖率分析应该作为参考，不能作为唯一判断依据。

## AI测试结果的可信度判断

当你用AI来分析测试结果或缺陷信息时，需要对它的输出保持一定的怀疑态度。具体来说有几个常见的"不可信"信号。

**过于自信的根因分析。** AI在分析错误日志时，有时候会给出一个非常确定的结论——"根据日志判断，问题的根本原因是数据库连接池耗尽"。这个结论可能是对的，但也可能只是众多可能性中的一个。AI倾向于给出一个"最像样"的答案，而不是说"我不确定"。你需要自己判断它给的根因是否有充分的证据支撑。

**遗漏上下文的建议。** AI给出的测试建议是基于你提供的信息做的。如果你没有告诉它系统的部署架构、数据库类型、第三方依赖等信息，它的建议可能在技术层面不适用。比如它可能建议你"使用Redis缓存来解决性能问题"，但你们的技术栈根本没有Redis。

**貌似合理的伪测试。** 有些AI生成的测试用例看起来结构完整、逻辑清晰，但仔细想想其实没什么测试价值——它测的是一个不可能出错的场景，或者测试步骤和预期结果之间没有真正的验证关系。这种"伪测试"最容易蒙混过关，评审时要格外留意。

## 变更影响分析与智能回归策略

AI在变更影响分析方面有独特的优势。当某个模块的代码发生变更时，你可以把变更内容（diff信息）、系统架构描述、模块依赖关系给AI，让它帮你评估：这次变更可能影响哪些功能？需要回归测试的范围是什么？

这个场景下AI的价值在于它能快速处理大量的关联信息。人在做变更影响分析时，很容易因为对某些模块不熟悉而遗漏间接影响。AI虽然也不完美，但它至少会"机械地"把所有可能的关联路径都考虑一遍。

基于AI的影响分析，你可以制定更精准的回归策略。不再是"每次发版都把所有用例跑一遍"这种粗放方式，而是根据变更内容动态确定回归范围。这在迭代速度快、回归成本高的团队里特别有价值。

## 建立AI辅助测试的团队规范

如果你的团队打算系统性地引入AI辅助测试，我建议尽早建立一些基本规范。不需要搞一个大文档，几条核心原则就够了。

**原则一：AI输出必须经过人工评审。** 这条不能打折扣。不管AI生成的用例看起来多完善，都必须有人review后才能进入用例库。这跟代码review是一个道理——不是说AI的代码一定有问题，而是review这个环节本身就是质量保障的一部分。

**原则二：标注AI生成内容。** 在用例管理系统或文档中，标注哪些用例是AI生成的、哪些是人工编写的。这不是歧视AI，而是方便后续追溯和分析：AI生成的用例在实际执行中发现Bug的比例是多少？它的用例质量随着我们调整提示词有没有提升？这些数据能帮你持续优化AI的使用方式。

**原则三：共享和迭代Prompt模板。** 团队成员各自摸索提示词效率太低。把好用的Prompt模板沉淀下来，放到团队共享文档里，新人来了直接用。同时定期回顾这些模板，根据实际效果做调整。

**原则四：明确AI不介入的环节。** 团队需要达成共识：哪些测试决策不依赖AI的判断。比如发布前的Go/No-Go决策、安全测试的方案设计、线上问题的应急响应。这些高风险决策需要人的经验和判断力，AI只能提供参考信息。

质量保障的本质没变，变的只是保障手段多了一种。用好AI的前提是：信任但验证。
